{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Configurer le logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def send_tan_to_kafka(topic, api_url, fields={}):\n",
    "    # Kafka configuration\n",
    "    kafka_config = {\n",
    "        'bootstrap_servers': 'kafka1:9092',  # Update with your Kafka broker\n",
    "    }\n",
    "\n",
    "    # Initialize Kafka Producer\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Fetch data from TAN API\n",
    "        response = requests.get(api_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # For each entry in the data, process and send it to Kafka\n",
    "            for entry in data:\n",
    "                # Process fields based on the provided mapping\n",
    "                for field in fields:\n",
    "                    entry[fields[field]] = entry.pop(field, None)\n",
    "\n",
    "                # Send the data to Kafka\n",
    "                producer.send(topic, value=entry)\n",
    "                logger.info(f\"Sent: {entry}\")\n",
    "\n",
    "            # Ensure all messages are sent\n",
    "            producer.flush()\n",
    "            logger.info(f\"Sent {len(data)} records.\")\n",
    "        else:\n",
    "            logger.error(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request failed: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Kafka sending failed: {e}\")\n",
    "    finally:\n",
    "        producer.close()\n",
    "\n",
    "# API URL for the stop data (using f-string formatting)\n",
    "latitude = \"47.264\"\n",
    "longitude = \"-1.585\"\n",
    "api_url = f\"https://open.tan.fr/ewp/arrets.json/{latitude}/{longitude}\"\n",
    "\n",
    "# Field mappings from API response to Kafka data schema\n",
    "fields = {\n",
    "    \"codeLieu\": \"stop_code\",\n",
    "    \"libelle\": \"stop_name\",\n",
    "    \"distance\": \"stop_distance\",\n",
    "}\n",
    "\n",
    "# Example of sending the data to Kafka\n",
    "send_tan_to_kafka(\"tan_stops\", api_url, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/24 03:20:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ArrayType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKafkaTanData\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Schéma des données reçues de l'API TAN\u001b[39;00m\n\u001b[1;32m     12\u001b[0m schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     13\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     15\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_distance\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m---> 16\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mligne\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mArrayType\u001b[49m(StructType([\n\u001b[1;32m     17\u001b[0m         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumLigne\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m     ])), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m ])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Connexion à Kafka et lecture des données\u001b[39;00m\n\u001b[1;32m     22\u001b[0m raw_stream \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread \\\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka1:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtan_stops\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;241m.\u001b[39mload()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ArrayType' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, regexp_extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, ArrayType\n",
    "\n",
    "\n",
    "# Assurez-vous que SparkSession est initialisé\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaTanData\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\") \\\n",
    "    .getOrCreate()\n",
    "# Schéma des données reçues de l'API TAN\n",
    "schema = StructType([\n",
    "    StructField(\"stop_code\", StringType(), True),\n",
    "    StructField(\"stop_name\", StringType(), True),\n",
    "    StructField(\"stop_distance\", StringType(), True),\n",
    "    StructField(\"ligne\", ArrayType(StructType([\n",
    "        StructField(\"numLigne\", StringType(), True)\n",
    "    ])), True)\n",
    "])\n",
    "\n",
    "# Connexion à Kafka et lecture des données\n",
    "raw_stream = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_stops\") \\\n",
    "    .load()\n",
    "\n",
    "# Traitez les données reçues depuis Kafka\n",
    "parsed_stream = raw_stream.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.stop_code\", \"data.stop_name\", \"data.stop_distance\")\n",
    "\n",
    "# Extraire la partie numérique de 'stop_distance' et la convertir en float\n",
    "parsed_stream = parsed_stream.withColumn(\n",
    "    \"stop_distance\",\n",
    "    regexp_extract(\"stop_distance\", r\"(\\d+(\\.\\d+)?)\", 1).cast(FloatType())  # Extraction numérique et conversion\n",
    ")\n",
    "\n",
    "# Affichez les résultats pour vérifier\n",
    "parsed_stream.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lecture des données du topic Kafka en mode batch\n",
    "# Nous utilisons Spark pour récupérer les données du topic Kafka\n",
    "batch_data = parsed_stream.collect()  # Collecte les données du DataFrame Spark en local\n",
    "\n",
    "# Convertir les données en Pandas DataFrame pour utiliser Pandas et Seaborn\n",
    "batch_df = pd.DataFrame(batch_data, columns=[\"stop_code\", \"stop_name\", \"stop_distance\"])\n",
    "\n",
    "# Traitement de la colonne stop_distance (convertir en float)\n",
    "batch_df['stop_distance'] = batch_df['stop_distance'].apply(lambda x: float(x.replace(' m', '') if isinstance(x, str) else x))\n",
    "\n",
    "# Visualisation des résultats avec Seaborn (par exemple, une distribution des distances des arrêts)\n",
    "sns.histplot(batch_df['stop_distance'], kde=True)\n",
    "plt.title(\"Distribution des distances des arrêts\")\n",
    "plt.xlabel(\"Distance (m)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, regexp_extract, count, avg, min, max, window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "# Initialiser SparkSession\n",
    "spark = SparkSession.builder.appName(\"KafkaTanData\").getOrCreate()\n",
    "\n",
    "# Schéma des données TAN\n",
    "schema = StructType([\n",
    "    StructField(\"stop_code\", StringType(), True),\n",
    "    StructField(\"stop_name\", StringType(), True),\n",
    "    StructField(\"stop_distance\", StringType(), True)  # Initialement String pour extraire la partie numérique\n",
    "])\n",
    "\n",
    "# Lecture depuis Kafka\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_stops\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformation des données\n",
    "parsed_stream = raw_stream.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.stop_code\", \"data.stop_name\", \"data.stop_distance\")\n",
    "\n",
    "# Extraction et conversion de la distance\n",
    "parsed_stream = parsed_stream.withColumn(\n",
    "    \"stop_distance\",\n",
    "    regexp_extract(\"stop_distance\", r\"(\\d+(\\.\\d+)?)\", 1).cast(FloatType())  # Extraction et conversion en float\n",
    ")\n",
    "\n",
    "#  Calcul des statistiques par fenêtre de 5 minutes\n",
    "stats_stream = parsed_stream \\\n",
    "    .groupBy(window(col(\"stop_code\"), \"5 minutes\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"nombre_arrets\"),  # Nombre d'arrêts reçus\n",
    "        avg(\"stop_distance\").alias(\"distance_moyenne\"),  # Distance moyenne\n",
    "        min(\"stop_distance\").alias(\"distance_minimale\"),  # Distance minimale\n",
    "        max(\"stop_distance\").alias(\"distance_maximale\")  # Distance maximale\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"debut_fenetre\"),\n",
    "        col(\"window.end\").alias(\"fin_fenetre\"),\n",
    "        col(\"nombre_arrets\"),\n",
    "        col(\"distance_moyenne\"),\n",
    "        col(\"distance_minimale\"),\n",
    "        col(\"distance_maximale\")\n",
    "    )\n",
    "\n",
    "# Écriture dans la console\n",
    "query = stats_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, regexp_extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "\n",
    "\n",
    "# Assurez-vous que SparkSession est initialisé\n",
    "spark = SparkSession.builder.appName(\"KafkaTanData\").getOrCreate()\n",
    "\n",
    "# Schéma des données reçues de l'API TAN\n",
    "schema = StructType([\n",
    "    StructField(\"stop_code\", StringType(), True),\n",
    "    StructField(\"stop_name\", StringType(), True),\n",
    "    StructField(\"stop_distance\", StringType(), True),  # Garder comme String pour l'extraction\n",
    "])\n",
    "\n",
    "# Connexion à Kafka et lecture des données\n",
    "raw_stream = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_stops\") \\\n",
    "    .load()\n",
    "\n",
    "# Traitez les données reçues depuis Kafka\n",
    "parsed_stream = raw_stream.selectExpr(\"CAST(value AS STRING) AS message\") \\\n",
    "    .select(from_json(col(\"message\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.stop_code\", \"data.stop_name\", \"data.stop_distance\")\n",
    "\n",
    "# Extraire la partie n\n",
    "# umérique de 'stop_distance' et la convertir en float\n",
    "parsed_stream = parsed_stream.withColumn(\n",
    "    \"stop_distance\",\n",
    "    regexp_extract(\"stop_distance\", r\"(\\d+(\\.\\d+)?)\", 1).cast(FloatType())  # Extraction numérique et conversion\n",
    ")\n",
    "\n",
    "# Affichez les résultats pour vérifier\n",
    "parsed_stream.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
