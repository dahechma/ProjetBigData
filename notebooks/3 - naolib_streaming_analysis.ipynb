{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naolib Streaming Analysis\n",
    "\n",
    "This notebook performs streaming analysis on real-time Naolib transportation data using a simplified approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/25 01:00:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/25 01:00:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/03/25 01:00:41 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/03/25 01:00:41 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "from kafka import KafkaConsumer\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('NaolibStreamingAnalysis') \\\n",
    "    .master('local[*]') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Function to Collect Data from Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Kafka configurations\n",
    "kafka_topic = \"naolib_realtime\"\n",
    "kafka_server = \"kafka1:9092\"\n",
    "\n",
    "# Function to convert wait time text to minutes\n",
    "def convert_wait_time(wt):\n",
    "    if pd.isna(wt):\n",
    "        return None\n",
    "    if wt == \"proche\":\n",
    "        return 0\n",
    "    # Try to extract numbers\n",
    "    if isinstance(wt, str):\n",
    "        # If it's just a number\n",
    "        if wt.isdigit():\n",
    "            return int(wt)\n",
    "        # If it's in format \"XYmn\"\n",
    "        match = re.search(r'(\\d+)', wt)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "# Function to collect real-time data from Kafka\n",
    "def collect_realtime_data(max_messages=50, timeout=10):\n",
    "    \"\"\"\n",
    "    Collect real-time data from Kafka\n",
    "    max_messages: Maximum number of messages to collect\n",
    "    timeout: Time to wait for messages in seconds\n",
    "    \"\"\"\n",
    "    print(f\"Collecting up to {max_messages} messages from Kafka...\")\n",
    "    consumer = KafkaConsumer(\n",
    "        kafka_topic,\n",
    "        bootstrap_servers=kafka_server,\n",
    "        auto_offset_reset='earliest',\n",
    "        consumer_timeout_ms=timeout*1000\n",
    "    )\n",
    "    \n",
    "    messages = []\n",
    "    expanded_rows = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for message in consumer:\n",
    "        try:\n",
    "            msg_data = json.loads(message.value.decode('utf-8'))\n",
    "            messages.append(msg_data)\n",
    "            \n",
    "            # Extract data from the message\n",
    "            timestamp = msg_data.get('timestamp')\n",
    "            stop_code = msg_data.get('stop_code')\n",
    "            stop_name = msg_data.get('stop_name')\n",
    "            \n",
    "            # Process arrivals array\n",
    "            arrivals = msg_data.get('arrivals', [])\n",
    "            for arrival in arrivals:\n",
    "                new_row = {\n",
    "                    'timestamp': timestamp,\n",
    "                    'stop_code': stop_code,\n",
    "                    'stop_name': stop_name,\n",
    "                    'direction': arrival.get('sens'),\n",
    "                    'terminus': arrival.get('terminus'),\n",
    "                    'wait_time_text': arrival.get('temps'),\n",
    "                    'is_real_time': arrival.get('tempsReel'),\n",
    "                    'line_number': arrival.get('ligne', {}).get('numLigne'),\n",
    "                    'processing_time': pd.Timestamp.now()\n",
    "                }\n",
    "                expanded_rows.append(new_row)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {str(e)}\")\n",
    "        \n",
    "        if len(messages) >= max_messages:\n",
    "            break\n",
    "            \n",
    "        if time.time() - start_time > timeout:\n",
    "            break\n",
    "    \n",
    "    consumer.close()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    if expanded_rows:\n",
    "        df = pd.DataFrame(expanded_rows)\n",
    "        df['wait_time_minutes'] = df['wait_time_text'].apply(convert_wait_time)\n",
    "        print(f\"Collected {len(messages)} messages with {len(expanded_rows)} arrivals\")\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data collected\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Streaming Analysis 1: Real-time Average Wait Times\n",
    "\n",
    "Our first streaming analysis calculates real-time average wait times with a 10-minute sliding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting up to 50 messages from Kafka...\n",
      "Collected 6 messages with 70 arrivals\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'temps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m analysis\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# ExÃ©cution\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m wait_stats \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_realtime_wait_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36manalyze_realtime_wait_times\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Nettoyage et conversion\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait_time_minutes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(convert_wait_time)\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait_time_minutes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Analyse par ligne\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temps'"
     ]
    }
   ],
   "source": [
    "def analyze_realtime_wait_times():\n",
    "    \"\"\"Analyse des temps d'attente en temps rÃ©el avec meilleure visualisation\"\"\"\n",
    "    \n",
    "    # 1. Collecte des donnÃ©es\n",
    "    data = collect_realtime_data(max_messages=50, timeout=10)\n",
    "    \n",
    "    if data.empty:\n",
    "        print(\"âš ï¸ Aucune donnÃ©e reÃ§ue de Kafka\")\n",
    "        return None\n",
    "    \n",
    "    # 2. Nettoyage et conversion\n",
    "    data['wait_time_minutes'] = data['temps'].apply(convert_wait_time)\n",
    "    data = data.dropna(subset=['wait_time_minutes'])\n",
    "    \n",
    "    # 3. Analyse par ligne\n",
    "    analysis = data.groupby('numLigne').agg(\n",
    "        avg_wait=('wait_time_minutes', 'mean'),\n",
    "        std_wait=('wait_time_minutes', 'std'),\n",
    "        min_wait=('wait_time_minutes', 'min'),\n",
    "        max_wait=('wait_time_minutes', 'max'),\n",
    "        count=('numLigne', 'count')\n",
    "    ).sort_values('avg_wait', ascending=False)\n",
    "    \n",
    "    # 4. Affichage dÃ©taillÃ©\n",
    "    print(f\"\\nðŸ“Š Analyse des temps d'attente ({len(data)} observations valides)\")\n",
    "    print(f\"PÃ©riode : {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if not analysis.empty:\n",
    "        # Affichage tableau\n",
    "        display(analysis.style\n",
    "                .background_gradient(cmap='YlOrRd', subset=['avg_wait'])\n",
    "                .format({'avg_wait': '{:.1f} min', 'std_wait': '{:.1f}'}))\n",
    "        \n",
    "        # Visualisation\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(\n",
    "            x='numLigne', \n",
    "            y='wait_time_minutes',\n",
    "            data=data,\n",
    "            palette='viridis'\n",
    "        )\n",
    "        plt.title(\"Distribution des temps d'attente par ligne\", pad=20)\n",
    "        plt.xlabel(\"Ligne de bus\")\n",
    "        plt.ylabel(\"Temps d'attente (minutes)\")\n",
    "        plt.grid(axis='y', alpha=0.3)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Aucune donnÃ©e valide aprÃ¨s filtrage\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# ExÃ©cution\n",
    "wait_stats = analyze_realtime_wait_times()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous Monitoring\n",
    "\n",
    "Let's run the analysis continuously to simulate streaming with sliding windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/3\n",
      "Collecting up to 50 messages from Kafka...\n",
      "Collected 6 messages with 70 arrivals\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'temps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temps'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43manalyze_realtime_wait_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_iterations \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWaiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterval_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds for next analysis...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m, in \u001b[0;36manalyze_realtime_wait_times\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 2. Nettoyage et conversion\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait_time_minutes\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(convert_wait_time)\n\u001b[1;32m     13\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait_time_minutes\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Analyse par ligne\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'temps'"
     ]
    }
   ],
   "source": [
    "# Number of iterations to run\n",
    "num_iterations = 3\n",
    "interval_seconds = 60\n",
    "\n",
    "try:\n",
    "    for i in range(num_iterations):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Iteration {i+1}/{num_iterations}\")\n",
    "        analyze_realtime_wait_times()\n",
    "        \n",
    "        if i < num_iterations - 1:\n",
    "            print(f\"\\nWaiting {interval_seconds} seconds for next analysis...\")\n",
    "            time.sleep(interval_seconds)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nMonitoring stopped by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Analysis 2: Delay Detection\n",
    "\n",
    "Our second streaming analysis detects unusual delays using a 15-minute window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting up to 100 messages from Kafka...\n",
      "Collected 6 messages with 70 arrivals\n",
      "\n",
      "=== Real-time Delay Detection (15-minute window) ===\n",
      "Analysis time: 2025-03-25 01:09:51.525010\n",
      "Total delays detected: 0\n",
      "\n",
      "No significant delays detected.\n"
     ]
    }
   ],
   "source": [
    "# Define baseline wait time for delay detection\n",
    "typical_wait_time = 10  # minutes\n",
    "\n",
    "# Function to detect delays\n",
    "def detect_delays():\n",
    "    \"\"\"Detect unusual delays in real-time\"\"\"\n",
    "    \n",
    "    # Collect data\n",
    "    data = collect_realtime_data(max_messages=100, timeout=15)\n",
    "    if data.empty:\n",
    "        return\n",
    "    \n",
    "    # Mark delays - consider waits 50% above typical as delays\n",
    "    data['is_delayed'] = data['wait_time_minutes'] > (typical_wait_time * 1.5)\n",
    "    data['delay_minutes'] = data.apply(\n",
    "        lambda x: x['wait_time_minutes'] - typical_wait_time if x['is_delayed'] else 0, \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Group by line and stop to detect patterns\n",
    "    grouped = data.groupby(['line_number', 'stop_name']).agg(\n",
    "        max_wait_time=('wait_time_minutes', 'max'),\n",
    "        avg_wait_time=('wait_time_minutes', 'mean'),\n",
    "        observation_count=('wait_time_minutes', 'count'),\n",
    "        delayed_count=('is_delayed', 'sum'),\n",
    "        avg_delay_minutes=('delay_minutes', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Filter for significant delays (at least 2 delayed observations)\n",
    "    significant = grouped[grouped['delayed_count'] >= 2]\n",
    "    \n",
    "    # Add severity classification\n",
    "    significant['delay_severity'] = 'MINOR'\n",
    "    significant.loc[significant['avg_delay_minutes'] > 10, 'delay_severity'] = 'MODERATE'\n",
    "    significant.loc[significant['avg_delay_minutes'] > 20, 'delay_severity'] = 'SEVERE'\n",
    "    \n",
    "    # Sort by severity\n",
    "    significant = significant.sort_values('avg_delay_minutes', ascending=False)\n",
    "    \n",
    "    # Print results\n",
    "    now = pd.Timestamp.now()\n",
    "    print(f\"\\n=== Real-time Delay Detection (15-minute window) ===\")\n",
    "    print(f\"Analysis time: {now}\")\n",
    "    print(f\"Total delays detected: {data['is_delayed'].sum()}\")\n",
    "    \n",
    "    if not significant.empty:\n",
    "        print(\"\\nSignificant Delays:\")\n",
    "        for _, row in significant.iterrows():\n",
    "            print(f\"DELAY ALERT: Line {row['line_number']} at {row['stop_name']} - {row['delay_severity']} delay of\"\n",
    "                  f\" {row['avg_delay_minutes']:.1f} minutes\")\n",
    "    else:\n",
    "        print(\"\\nNo significant delays detected.\")\n",
    "        \n",
    "    return significant\n",
    "\n",
    "# Run the delay detection\n",
    "delays = detect_delays()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Continuous Delay Monitoring\n",
    "\n",
    "Let's monitor for delays continuously to simulate a real-time alert system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3/3\n",
      "Collecting up to 100 messages from Kafka...\n"
     ]
    },
    {
     "ename": "NoBrokersAvailable",
     "evalue": "NoBrokersAvailable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdetect_delays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_iterations \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWaiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterval_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds for next detection...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m, in \u001b[0;36mdetect_delays\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect unusual delays in real-time\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Collect data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_realtime_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_messages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m, in \u001b[0;36mcollect_realtime_data\u001b[0;34m(max_messages, timeout)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mCollect real-time data from Kafka\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03mmax_messages: Maximum number of messages to collect\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03mtimeout: Time to wait for messages in seconds\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollecting up to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_messages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m messages from Kafka...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m consumer \u001b[38;5;241m=\u001b[39m \u001b[43mKafkaConsumer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkafka_topic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbootstrap_servers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkafka_server\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_offset_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mearliest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconsumer_timeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\n\u001b[1;32m     35\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m messages \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m expanded_rows \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/kafka/consumer/group.py:379\u001b[0m, in \u001b[0;36mKafkaConsumer.__init__\u001b[0;34m(self, *topics, **configs)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, str_version\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m    376\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse api_version=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m [tuple] -- \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m as str is deprecated\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    377\u001b[0m                 \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m]), str_version)\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkafka_client\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Get auto-discovered / normalized version from client\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/kafka/client_async.py:262\u001b[0m, in \u001b[0;36mKafkaClient.__init__\u001b[0;34m(self, **configs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Check Broker Version if not set explicitly\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m BROKER_API_VERSIONS:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_versions \u001b[38;5;241m=\u001b[39m BROKER_API_VERSIONS[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_version\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/kafka/client_async.py:1064\u001b[0m, in \u001b[0;36mKafkaClient.check_version\u001b[0;34m(self, node_id, timeout, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNodeNotReadyError(node_id)\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Errors\u001b[38;5;241m.\u001b[39mNoBrokersAvailable()\n",
      "\u001b[0;31mNoBrokersAvailable\u001b[0m: NoBrokersAvailable"
     ]
    }
   ],
   "source": [
    "# Number of iterations to run\n",
    "num_iterations = 3\n",
    "interval_seconds = 120\n",
    "\n",
    "try:\n",
    "    for i in range(num_iterations):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Iteration {i+1}/{num_iterations}\")\n",
    "        detect_delays()\n",
    "        \n",
    "        if i < num_iterations - 1:\n",
    "            print(f\"\\nWaiting {interval_seconds} seconds for next detection...\")\n",
    "            time.sleep(interval_seconds)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nMonitoring stopped by user.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
