{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Big Data - Analyse Batch des Données TAN avec Spark\n",
    "\n",
    "Ce notebook permet d'analyser en mode batch les données collectées depuis l'API TAN et stockées dans Kafka. Nous effectuerons deux analyses distinctes :\n",
    "\n",
    "1. Distribution des arrêts de transport et leurs caractéristiques\n",
    "2. Analyse des temps d'attente par ligne et par arrêt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation de Spark\n",
    "\n",
    "Commençons par configurer notre session Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-11-openjdk-amd64/bin/java: No such file or directory\n",
      "/opt/conda/lib/python3.11/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Création de la session Spark\u001b[39;00m\n\u001b[1;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTAN Batch Analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Réduire les messages de log\u001b[39;00m\n\u001b[1;32m     15\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 477\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 512\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, explode, regexp_extract, count, avg, min, max\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Création de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TAN Batch Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Réduire les messages de log\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Session Spark initialisée!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse 1 : Distribution des arrêts de transport\n",
    "\n",
    "Nous allons analyser la distribution des arrêts de bus/tram, leur distance et les lignes qu'ils desservent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du schéma et lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du schéma pour les données d'arrêts\n",
    "stop_schema = StructType([\n",
    "    StructField(\"stop_code\", StringType(), True),\n",
    "    StructField(\"stop_name\", StringType(), True),\n",
    "    StructField(\"stop_distance\", StringType(), True),\n",
    "    StructField(\"ligne\", ArrayType(StructType([\n",
    "        StructField(\"numLigne\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Lecture des données depuis Kafka en mode batch\n",
    "stops_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_stops\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "\n",
    "# Parsing du JSON\n",
    "parsed_stops = stops_df \\\n",
    "    .select(from_json(\"json\", stop_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Affichage d'un échantillon des données\n",
    "parsed_stops.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement des données\n",
    "\n",
    "Extrayons maintenant les informations pertinentes et convertissons la distance en valeur numérique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction de la partie numérique de la distance (ex: \"256 m\" -> 256)\n",
    "parsed_stops = parsed_stops \\\n",
    "    .withColumn(\"distance_meters\", \n",
    "                regexp_extract(\"stop_distance\", r\"(\\d+)\", 1).cast(FloatType()))\n",
    "\n",
    "# Extraction des numéros de ligne dans une colonne distincte\n",
    "stops_with_lines = parsed_stops \\\n",
    "    .withColumn(\"line_numbers\", explode(\"ligne\")) \\\n",
    "    .select(\"stop_code\", \"stop_name\", \"distance_meters\", \"line_numbers.numLigne\")\n",
    "\n",
    "# Mise en cache pour optimiser les accès multiples\n",
    "stops_with_lines.cache()\n",
    "\n",
    "# Affichage des données après traitement\n",
    "stops_with_lines.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse et visualisation 1: Distribution des arrêts par distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame Pandas pour visualisation\n",
    "stops_pandas = stops_with_lines.toPandas()\n",
    "\n",
    "# Statistiques descriptives\n",
    "stats = stops_pandas[\"distance_meters\"].describe()\n",
    "print(\"Statistiques descriptives des distances:\")\n",
    "print(stats)\n",
    "\n",
    "# Visualisation: Distribution des arrêts par distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=stops_pandas, x=\"distance_meters\", bins=20, kde=True)\n",
    "plt.title(\"Distribution des arrêts par distance\")\n",
    "plt.xlabel(\"Distance (mètres)\")\n",
    "plt.ylabel(\"Nombre d'arrêts\")\n",
    "plt.axvline(x=stops_pandas[\"distance_meters\"].mean(), color='r', linestyle='--', \n",
    "            label=f'Moyenne: {stops_pandas[\"distance_meters\"].mean():.1f}m')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse et visualisation 2: Nombre d'arrêts par ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comptage des arrêts par ligne\n",
    "line_counts = stops_pandas[\"numLigne\"].value_counts().reset_index()\n",
    "line_counts.columns = ['Ligne', 'Nombre_arrets']\n",
    "line_counts = line_counts.sort_values(by='Nombre_arrets', ascending=False).head(10)\n",
    "\n",
    "# Visualisation: Top 10 des lignes par nombre d'arrêts\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=line_counts, x='Ligne', y='Nombre_arrets')\n",
    "plt.title(\"Top 10 des lignes par nombre d'arrêts desservis\")\n",
    "plt.xlabel(\"Ligne\")\n",
    "plt.ylabel(\"Nombre d'arrêts\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse 2 : Temps d'attente aux arrêts\n",
    "\n",
    "Analysons maintenant les temps d'attente aux différents arrêts et pour différentes lignes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du schéma et lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du schéma pour les données de temps d'attente\n",
    "wait_schema = StructType([\n",
    "    StructField(\"ligne\", StructType([\n",
    "        StructField(\"numLigne\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"terminus\", StringType(), True),\n",
    "    StructField(\"arret\", StructType([\n",
    "        StructField(\"codeArret\", StringType(), True),\n",
    "        StructField(\"libelle\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"temps\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Lecture des données depuis Kafka en mode batch\n",
    "wait_df = spark.read \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_wait_times\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .option(\"endingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\")\n",
    "\n",
    "# Parsing du JSON\n",
    "parsed_wait = wait_df \\\n",
    "    .select(from_json(\"json\", wait_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Affichage d'un échantillon des données\n",
    "parsed_wait.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplatissement de la structure imbriquée\n",
    "flattened_wait = parsed_wait \\\n",
    "    .select(\n",
    "        col(\"ligne.numLigne\").alias(\"line_number\"),\n",
    "        col(\"terminus\").alias(\"destination\"),\n",
    "        col(\"arret.codeArret\").alias(\"stop_code\"),\n",
    "        col(\"arret.libelle\").alias(\"stop_name\"),\n",
    "        col(\"temps\").alias(\"wait_time\"),\n",
    "        col(\"timestamp\")\n",
    "    )\n",
    "\n",
    "# Extraction de la valeur numérique du temps d'attente (ex: \"5 mn\" -> 5)\n",
    "flattened_wait = flattened_wait \\\n",
    "    .withColumn(\"wait_minutes\", \n",
    "                regexp_extract(\"wait_time\", r\"(\\d+)\", 1).cast(FloatType()))\n",
    "\n",
    "# Mise en cache pour optimiser les accès multiples\n",
    "flattened_wait.cache()\n",
    "\n",
    "# Affichage des données après traitement\n",
    "flattened_wait.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse et visualisation 1: Temps d'attente moyen par ligne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du temps d'attente moyen par ligne\n",
    "wait_by_line = flattened_wait.groupBy(\"line_number\") \\\n",
    "    .agg(\n",
    "        avg(\"wait_minutes\").alias(\"avg_wait\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    ) \\\n",
    "    .filter(col(\"count\") > 3)  # Au moins 3 observations par ligne\n",
    "    \n",
    "# Conversion en DataFrame Pandas\n",
    "wait_by_line_pandas = wait_by_line.toPandas()\n",
    "\n",
    "# Tri par temps d'attente moyen décroissant\n",
    "wait_by_line_pandas = wait_by_line_pandas.sort_values(by=\"avg_wait\", ascending=False)\n",
    "\n",
    "# Visualisation: Temps d'attente moyen par ligne\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=wait_by_line_pandas.head(10), x=\"line_number\", y=\"avg_wait\")\n",
    "plt.title(\"Temps d'attente moyen par ligne\")\n",
    "plt.xlabel(\"Ligne\")\n",
    "plt.ylabel(\"Temps d'attente moyen (minutes)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse et visualisation 2: Distribution des temps d'attente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en DataFrame Pandas pour visualisation\n",
    "wait_pandas = flattened_wait.toPandas()\n",
    "\n",
    "# Statistiques descriptives\n",
    "wait_stats = wait_pandas[\"wait_minutes\"].describe()\n",
    "print(\"Statistiques descriptives des temps d'attente:\")\n",
    "print(wait_stats)\n",
    "\n",
    "# Visualisation: Distribution des temps d'attente\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=wait_pandas, x=\"wait_minutes\", bins=15, kde=True)\n",
    "plt.title(\"Distribution des temps d'attente\")\n",
    "plt.xlabel(\"Temps d'attente (minutes)\")\n",
    "plt.ylabel(\"Fréquence\")\n",
    "plt.axvline(x=wait_pandas[\"wait_minutes\"].mean(), color='r', linestyle='--', \n",
    "            label=f'Moyenne: {wait_pandas[\"wait_minutes\"].mean():.1f} min')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse et visualisation 3: Temps d'attente par arrêt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du temps d'attente moyen par arrêt\n",
    "wait_by_stop = flattened_wait.groupBy(\"stop_name\") \\\n",
    "    .agg(\n",
    "        avg(\"wait_minutes\").alias(\"avg_wait\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    ) \\\n",
    "    .filter(col(\"count\") > 3)  # Au moins 3 observations par arrêt\n",
    "    \n",
    "# Conversion en DataFrame Pandas\n",
    "wait_by_stop_pandas = wait_by_stop.toPandas()\n",
    "\n",
    "# Tri par temps d'attente moyen décroissant\n",
    "wait_by_stop_pandas = wait_by_stop_pandas.sort_values(by=\"avg_wait\", ascending=False)\n",
    "\n",
    "# Visualisation: Top 10 des arrêts avec le temps d'attente le plus long\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(data=wait_by_stop_pandas.head(10), x=\"stop_name\", y=\"avg_wait\")\n",
    "plt.title(\"Top 10 des arrêts avec le temps d'attente le plus long\")\n",
    "plt.xlabel(\"Arrêt\")\n",
    "plt.ylabel(\"Temps d'attente moyen (minutes)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthèse des résultats et insights\n",
    "\n",
    "Récapitulons les principales découvertes de notre analyse batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques générales\n",
    "print(\"=== Statistiques sur les arrêts ===\")\n",
    "print(f\"Nombre total d'arrêts uniques: {stops_pandas['stop_name'].nunique()}\")\n",
    "print(f\"Distance moyenne des arrêts: {stops_pandas['distance_meters'].mean():.2f} mètres\")\n",
    "print(f\"Distance médiane: {stops_pandas['distance_meters'].median():.2f} mètres\")\n",
    "print(f\"Distance minimale: {stops_pandas['distance_meters'].min():.2f} mètres\")\n",
    "print(f\"Distance maximale: {stops_pandas['distance_meters'].max():.2f} mètres\")\n",
    "\n",
    "print(\"\\n=== Statistiques sur les temps d'attente ===\")\n",
    "print(f\"Temps d'attente moyen: {wait_pandas['wait_minutes'].mean():.2f} minutes\")\n",
    "print(f\"Temps d'attente médian: {wait_pandas['wait_minutes'].median():.2f} minutes\")\n",
    "print(f\"Temps d'attente minimal: {wait_pandas['wait_minutes'].min():.2f} minutes\")\n",
    "print(f\"Temps d'attente maximal: {wait_pandas['wait_minutes'].max():.2f} minutes\")\n",
    "\n",
    "print(\"\\n=== Top 3 des lignes les plus fréquentes ===\")\n",
    "for i, row in line_counts.head(3).iterrows():\n",
    "    print(f\"Ligne {row['Ligne']}: {row['Nombre_arrets']} arrêts\")\n",
    "\n",
    "print(\"\\n=== Top 3 des lignes avec le temps d'attente le plus long ===\")\n",
    "for i, row in wait_by_line_pandas.head(3).iterrows():\n",
    "    print(f\"Ligne {row['line_number']}: {row['avg_wait']:.2f} minutes en moyenne\")\n",
    "\n",
    "print(\"\\n=== Top 3 des arrêts avec le temps d'attente le plus long ===\")\n",
    "for i, row in wait_by_stop_pandas.head(3).iterrows():\n",
    "    print(f\"{row['stop_name']}: {row['avg_wait']:.2f} minutes en moyenne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Cette analyse batch nous a permis de mieux comprendre la distribution spatiale des arrêts de transport en commun à Nantes ainsi que les tendances en termes de temps d'attente. Ces informations sont précieuses pour :\n",
    "\n",
    "1. Identifier les zones de la ville bien ou mal desservies par les transports en commun\n",
    "2. Repérer les lignes et arrêts qui pourraient bénéficier d'une amélioration de la fréquence\n",
    "3. Aider les voyageurs à mieux planifier leurs déplacements en fonction des temps d'attente moyens\n",
    "\n",
    "Dans le notebook suivant, nous explorerons ces données en streaming pour obtenir des analyses en temps réel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libérer les ressources\n",
    "stops_with_lines.unpersist()\n",
    "flattened_wait.unpersist()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
