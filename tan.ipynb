{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'ligne': [{'numLigne': '109'}, {'numLigne': '116'}, {'numLigne': '2'}, {'numLigne': '2B'}, {'numLigne': '50'}, {'numLigne': '59'}, {'numLigne': '89'}, {'numLigne': 'C2'}], 'stop_code': 'LCAR', 'stop_name': 'Le Cardo', 'stop_distance': '256 m'}\n",
      "Sent: {'ligne': [{'numLigne': '59'}], 'stop_code': 'AURR', 'stop_name': 'Aurore', 'stop_distance': '324 m'}\n",
      "Sent: {'ligne': [{'numLigne': '116'}, {'numLigne': '2B'}, {'numLigne': '50'}, {'numLigne': '59'}, {'numLigne': 'C2'}], 'stop_code': 'LRHE', 'stop_name': 'Les Roches', 'stop_distance': '428 m'}\n",
      "Sent: {'ligne': [{'numLigne': '109'}, {'numLigne': '116'}, {'numLigne': '2B'}, {'numLigne': '50'}, {'numLigne': '89'}], 'stop_code': 'CORA', 'stop_name': 'Conraie', 'stop_distance': '430 m'}\n",
      "Sent: {'ligne': [{'numLigne': '59'}], 'stop_code': 'BDLA', 'stop_name': 'Bout des Landes', 'stop_distance': '432 m'}\n",
      "Sent 5 records.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "def send_tan_to_kafka(topic, api_url, fields={}):\n",
    "    # Kafka configuration\n",
    "    kafka_config = {\n",
    "        'bootstrap_servers': 'kafka1:9092',  # Update with your Kafka broker\n",
    "    }\n",
    "\n",
    "    # Initialize Kafka Producer\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=kafka_config['bootstrap_servers'],\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "\n",
    "    # Fetch data from TAN API\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # For each entry in the data, process and send it to Kafka\n",
    "        for entry in data:\n",
    "            # Process fields based on the provided mapping\n",
    "            for field in fields:\n",
    "                entry[fields[field]] = entry.pop(field, None)\n",
    "\n",
    "            # Send the data to Kafka\n",
    "            producer.send(topic, value=entry)\n",
    "            print(f\"Sent: {entry}\")\n",
    "\n",
    "        # Ensure all messages are sent\n",
    "        producer.flush()\n",
    "        print(f\"Sent {len(data)} records.\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "\n",
    "# API URL for the stop data (using f-string formatting)\n",
    "latitude = \"47.264\"\n",
    "longitude = \"-1.585\"\n",
    "api_url = f\"https://open.tan.fr/ewp/arrets.json/{latitude}/{longitude}\"\n",
    "\n",
    "# Field mappings from API response to Kafka data schema\n",
    "fields = {\n",
    "    \"codeLieu\": \"stop_code\",\n",
    "    \"libelle\": \"stop_name\",\n",
    "    \"distance\": \"stop_distance\",\n",
    "}\n",
    "\n",
    "# Example of sending the data to Kafka\n",
    "send_tan_to_kafka(\"tan_stops\", api_url, fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kafka Consumer pour r√©cup√©rer les donn√©es batch\n",
    "consumer = KafkaConsumer(\n",
    "    \"tan_stops\", \n",
    "    bootstrap_servers=\"kafka1:9092\",\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# Stocker les messages Kafka dans une liste\n",
    "data = [message.value for message in consumer]\n",
    "\n",
    "# Convertir en DataFrame Pandas\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# V√©rifier la structure des donn√©es\n",
    "print(df.head())\n",
    "\n",
    "# Enregistrer en CSV pour analyse (optionnel)\n",
    "df.to_csv(\"tan_stops_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"stop_distance\"].astype(float), bins=30, kde=True)\n",
    "plt.xlabel(\"Distance (m√®tres)\")\n",
    "plt.ylabel(\"Nombre d'arr√™ts\")\n",
    "plt.title(\"R√©partition des arr√™ts par distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stops = df[\"stop_name\"].value_counts().head(10)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=top_stops.index, y=top_stops.values, palette=\"viridis\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Nom des arr√™ts\")\n",
    "plt.ylabel(\"Nombre d'apparitions\")\n",
    "plt.title(\"Top 10 des arr√™ts les plus fr√©quents\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window\n",
    "from pyspark.sql.types import StructType, StringType\n",
    "\n",
    "# Cr√©ation de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaSparkStreaming\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# D√©finition du sch√©ma des donn√©es\n",
    "schema = StructType() \\\n",
    "    .add(\"stop_code\", StringType()) \\\n",
    "    .add(\"stop_name\", StringType()) \\\n",
    "    .add(\"stop_distance\", StringType())\n",
    "\n",
    "# Lecture en streaming depuis Kafka\n",
    "df_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_stops\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformation des donn√©es\n",
    "df_parsed = df_stream \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# **üìå √âtape 2 : Fen√™tre temporelle sur les arr√™ts de bus**\n",
    "df_windowed = df_parsed \\\n",
    "    .withWatermark(\"timestamp\", \"5 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\"), \n",
    "        col(\"stop_name\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Affichage des r√©sultats dans la console en temps r√©el\n",
    "query = df_windowed.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sch√©ma pour les temps d'attente\n",
    "schema_wait = StructType() \\\n",
    "    .add(\"codeArret\", StringType()) \\\n",
    "    .add(\"temps\", StringType()) \\\n",
    "    .add(\"numLigne\", StringType())\n",
    "\n",
    "# Lecture en streaming des temps d‚Äôattente\n",
    "df_wait_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_wait_times\") \\\n",
    "    .load()\n",
    "\n",
    "# Transformation des donn√©es\n",
    "df_wait_parsed = df_wait_stream \\\n",
    "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "    .select(from_json(col(\"value\"), schema_wait).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Fen√™tre temporelle pour regrouper les temps d'attente sur 10 minutes\n",
    "df_wait_windowed = df_wait_parsed \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"10 minutes\"), \n",
    "        col(\"numLigne\")\n",
    "    ) \\\n",
    "    .avg(\"temps\")\n",
    "\n",
    "# Affichage en streaming\n",
    "query_wait = df_wait_windowed.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query_wait.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
