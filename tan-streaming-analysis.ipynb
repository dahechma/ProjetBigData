{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Big Data - Analyse Streaming des Données TAN avec Spark\n",
    "\n",
    "Ce notebook permet d'analyser en temps réel (streaming) les données collectées depuis l'API TAN et envoyées à Kafka. Nous effectuerons deux analyses distinctes en utilisant des fenêtres temporelles :\n",
    "\n",
    "1. Surveillance en temps réel des arrêts de transport avec fenêtres glissantes\n",
    "2. Analyse des tendances de temps d'attente par fenêtres temporelles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation de Spark\n",
    "\n",
    "Commençons par configurer notre session Spark pour le traitement en streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/pyspark/bin/spark-class: line 71: /usr/lib/jvm/java-17-openjdk-amd64/bin/java: No such file or directory\n",
      "/opt/conda/lib/python3.12/site-packages/pyspark/bin/spark-class: line 97: CMD: bad array subscript\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Création de la session Spark\u001b[39;00m\n\u001b[1;32m      8\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTAN Streaming Analysis\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.jars.packages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Réduire les messages de log\u001b[39;00m\n\u001b[1;32m     14\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetLogLevel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, window, expr, to_timestamp\n",
    "from pyspark.sql.functions import avg, count, min, max, sum, explode, regexp_extract\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType, TimestampType\n",
    "import time\n",
    "\n",
    "# Création de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TAN Streaming Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Réduire les messages de log\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "print(\"Session Spark initialisée pour le streaming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Analysis 1: Surveillance des arrêts en temps réel\n",
    "\n",
    "Cette analyse nous permettra de surveiller les arrêts en temps réel et de calculer des statistiques sur des fenêtres de temps de 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du schéma et configuration du stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m stop_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m      3\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     10\u001b[0m ])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Création du stream pour la lecture depuis Kafka\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m stops_stream \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mreadStream \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka1:9092\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtan_stops\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstartingOffsets\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatest\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;241m.\u001b[39mload() \\\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING) AS json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp AS kafka_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Parsing du JSON\u001b[39;00m\n\u001b[1;32m     23\u001b[0m parsed_stops \u001b[38;5;241m=\u001b[39m stops_stream \\\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\n\u001b[1;32m     25\u001b[0m         from_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, stop_schema)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     26\u001b[0m         col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m     ) \\\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Définition du schéma pour les données d'arrêts\n",
    "stop_schema = StructType([\n",
    "    StructField(\"stop_code\", StringType(), True),\n",
    "    StructField(\"stop_name\", StringType(), True),\n",
    "    StructField(\"stop_distance\", StringType(), True),\n",
    "    StructField(\"ligne\", ArrayType(StructType([\n",
    "        StructField(\"numLigne\", StringType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Création du stream pour la lecture depuis Kafka\n",
    "stops_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_stops\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\", \"timestamp AS kafka_timestamp\")\n",
    "\n",
    "# Parsing du JSON\n",
    "parsed_stops = stops_stream \\\n",
    "    .select(\n",
    "        from_json(\"json\", stop_schema).alias(\"data\"),\n",
    "        col(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\"data.*\", \"kafka_timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement et analyse en streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation du timestamp en format timestamp et extraction de la distance numérique\n",
    "processed_stops = parsed_stops \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"distance_meters\", \n",
    "                regexp_extract(\"stop_distance\", r\"(\\d+)\", 1).cast(FloatType()))\n",
    "\n",
    "# Analyse par fenêtre temporelle: statistiques sur les arrêts par intervalles de 5 minutes\n",
    "stops_stats = processed_stops \\\n",
    "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_stops\"),\n",
    "        avg(\"distance_meters\").alias(\"avg_distance\"),\n",
    "        min(\"distance_meters\").alias(\"min_distance\"),\n",
    "        max(\"distance_meters\").alias(\"max_distance\"),\n",
    "        count(\"stop_code\").alias(\"num_data_points\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"total_stops\"),\n",
    "        col(\"avg_distance\"),\n",
    "        col(\"min_distance\"),\n",
    "        col(\"max_distance\"),\n",
    "        col(\"num_data_points\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Démarrage du premier streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et démarrage du stream pour afficher les résultats dans la console\n",
    "query1 = stops_stats \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query 1 démarrée. Analyse des arrêts par fenêtres de 5 minutes en cours...\")\n",
    "print(\"Ce streaming continuera jusqu'à ce que vous l'arrêtiez manuellement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous souhaitez arrêter le premier stream, exécutez la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêt du premier stream (facultatif)\n",
    "if 'query1' in locals() and query1.isActive:\n",
    "    query1.stop()\n",
    "    print(\"Premier stream arrêté.\")\n",
    "else:\n",
    "    print(\"Le premier stream n'est pas actif ou n'a pas été démarré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Analysis 2: Analyse des temps d'attente avec fenêtres glissantes\n",
    "\n",
    "Cette analyse nous permettra de suivre les temps d'attente par ligne en utilisant des fenêtres glissantes de 10 minutes se déplaçant toutes les 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du schéma et configuration du stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition du schéma pour les données de temps d'attente\n",
    "wait_schema = StructType([\n",
    "    StructField(\"ligne\", StructType([\n",
    "        StructField(\"numLigne\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"terminus\", StringType(), True),\n",
    "    StructField(\"arret\", StructType([\n",
    "        StructField(\"codeArret\", StringType(), True),\n",
    "        StructField(\"libelle\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"temps\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Création du stream pour la lecture depuis Kafka\n",
    "wait_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka1:9092\") \\\n",
    "    .option(\"subscribe\", \"tan_wait_times\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) AS json\", \"timestamp AS kafka_timestamp\")\n",
    "\n",
    "# Parsing du JSON\n",
    "parsed_wait = wait_stream \\\n",
    "    .select(\n",
    "        from_json(\"json\", wait_schema).alias(\"data\"),\n",
    "        col(\"kafka_timestamp\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"data.ligne.numLigne\").alias(\"line_number\"),\n",
    "        col(\"data.terminus\").alias(\"destination\"),\n",
    "        col(\"data.arret.codeArret\").alias(\"stop_code\"),\n",
    "        col(\"data.arret.libelle\").alias(\"stop_name\"),\n",
    "        col(\"data.temps\").alias(\"wait_time\"),\n",
    "        col(\"data.timestamp\"),\n",
    "        col(\"kafka_timestamp\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traitement et analyse en streaming avec fenêtres glissantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des minutes d'attente et conversion du timestamp\n",
    "processed_wait = parsed_wait \\\n",
    "    .withColumn(\"event_time\", to_timestamp(col(\"timestamp\"))) \\\n",
    "    .withColumn(\"wait_minutes\", \n",
    "                regexp_extract(\"wait_time\", r\"(\\d+)\", 1).cast(FloatType()))\n",
    "\n",
    "# Analyse par fenêtre glissante: temps d'attente moyen par ligne sur des fenêtres de 10 minutes glissant toutes les 5 minutes\n",
    "wait_stats_by_line = processed_wait \\\n",
    "    .withWatermark(\"event_time\", \"15 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"),\n",
    "        col(\"line_number\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"wait_minutes\").alias(\"avg_wait\"),\n",
    "        count(\"*\").alias(\"count\"),\n",
    "        min(\"wait_minutes\").alias(\"min_wait\"),\n",
    "        max(\"wait_minutes\").alias(\"max_wait\")\n",
    "    ) \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"avg_wait\"),\n",
    "        col(\"min_wait\"),\n",
    "        col(\"max_wait\"),\n",
    "        col(\"count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Démarrage du deuxième streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et démarrage du stream pour afficher les résultats dans la console\n",
    "query2 = wait_stats_by_line \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 15) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query 2 démarrée. Analyse des temps d'attente par fenêtres glissantes en cours...\")\n",
    "print(\"Ce streaming continuera jusqu'à ce que vous l'arrêtiez manuellement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous souhaitez arrêter le deuxième stream, exécutez la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêt du deuxième stream (facultatif)\n",
    "if 'query2' in locals() and query2.isActive:\n",
    "    query2.stop()\n",
    "    print(\"Deuxième stream arrêté.\")\n",
    "else:\n",
    "    print(\"Le deuxième stream n'est pas actif ou n'a pas été démarré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Analyse des tendances par arrêt avec fenêtres temporelles\n",
    "\n",
    "Pour une analyse plus détaillée, nous pouvons également suivre les temps d'attente par arrêt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par fenêtre temporelle: temps d'attente moyen par arrêt sur des fenêtres de 15 minutes\n",
    "wait_stats_by_stop = processed_wait \\\n",
    "    .withWatermark(\"event_time\", \"20 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"15 minutes\"),\n",
    "        col(\"stop_name\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"wait_minutes\").alias(\"avg_wait\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    ) \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"stop_name\"),\n",
    "        col(\"avg_wait\"),\n",
    "        col(\"count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et démarrage du stream pour afficher les résultats dans la console\n",
    "query3 = wait_stats_by_stop \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 15) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query 3 démarrée. Analyse des temps d'attente par arrêt en cours...\")\n",
    "print(\"Ce streaming continuera jusqu'à ce que vous l'arrêtiez manuellement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous souhaitez arrêter le troisième stream, exécutez la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêt du troisième stream (facultatif)\n",
    "if 'query3' in locals() and query3.isActive:\n",
    "    query3.stop()\n",
    "    print(\"Troisième stream arrêté.\")\n",
    "else:\n",
    "    print(\"Le troisième stream n'est pas actif ou n'a pas été démarré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse avancée: Détection des anomalies de temps d'attente\n",
    "\n",
    "Nous allons maintenant mettre en place une détection d'anomalies simple pour identifier les lignes ayant des temps d'attente inhabituellement longs par rapport à la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse par fenêtre glissante avec détection d'anomalies\n",
    "# On considère un temps d'attente comme anormal s'il dépasse 1.5 fois la moyenne calculée sur les dernières fenêtres\n",
    "wait_anomalies = processed_wait \\\n",
    "    .withWatermark(\"event_time\", \"20 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"10 minutes\", \"5 minutes\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"stop_name\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"wait_minutes\").alias(\"avg_wait\"),\n",
    "        max(\"wait_minutes\").alias(\"max_wait\"),\n",
    "        count(\"*\").alias(\"count\")\n",
    "    ) \\\n",
    "    .filter(col(\"count\") > 1) \\\n",
    "    .filter(col(\"max_wait\") > 15) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"stop_name\"),\n",
    "        col(\"avg_wait\"),\n",
    "        col(\"max_wait\"),\n",
    "        col(\"count\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration et démarrage du stream pour afficher les anomalies dans la console\n",
    "query4 = wait_anomalies \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query 4 démarrée. Détection d'anomalies en cours...\")\n",
    "print(\"Ce streaming continuera jusqu'à ce que vous l'arrêtiez manuellement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous souhaitez arrêter le quatrième stream, exécutez la cellule suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêt du quatrième stream (facultatif)\n",
    "if 'query4' in locals() and query4.isActive:\n",
    "    query4.stop()\n",
    "    print(\"Quatrième stream arrêté.\")\n",
    "else:\n",
    "    print(\"Le quatrième stream n'est pas actif ou n'a pas été démarré.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arrêt de tous les streams\n",
    "\n",
    "Pour arrêter tous les streams en cours, exécutez cette cellule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrêt de tous les streams\n",
    "for query_name in ['query1', 'query2', 'query3', 'query4']:\n",
    "    if query_name in locals() and eval(f\"{query_name}.isActive\"):\n",
    "        eval(f\"{query_name}.stop()\")\n",
    "        print(f\"{query_name} arrêté.\")\n",
    "\n",
    "print(\"Tous les streams sont maintenant arrêtés.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans ce notebook, nous avons mis en place plusieurs analyses en streaming sur les données de transport en commun de Nantes :\n",
    "\n",
    "1. **Surveillance des arrêts en temps réel** : Nous avons analysé les données d'arrêts sur des fenêtres temporelles de 5 minutes, ce qui nous permet de suivre l'évolution de la distribution des arrêts et des distances en temps réel.\n",
    "\n",
    "2. **Analyse des temps d'attente** : Nous avons utilisé des fenêtres glissantes de 10 minutes (se déplaçant toutes les 5 minutes) pour analyser l'évolution des temps d'attente par ligne, ce qui permet de détecter des tendances ou des problèmes de ponctualité.\n",
    "\n",
    "3. **Analyse par arrêt** : Nous avons également suivi les temps d'attente par arrêt, ce qui peut aider à identifier les stations ayant des problèmes récurrents de service.\n",
    "\n",
    "4. **Détection d'anomalies** : Enfin, nous avons mis en place une détection simple d'anomalies pour identifier les situations où les temps d'attente sont anormalement longs, ce qui pourrait indiquer des perturbations ou des incidents.\n",
    "\n",
    "Ces analyses en streaming fournissent une vue en temps réel du réseau de transport, ce qui est crucial pour la supervision opérationnelle et l'amélioration continue du service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
